{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555f7193",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "538f73ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of all categories (topics):\n",
      "['Chinh tri Xa hoi', 'Doi song', 'Khoa hoc', 'Kinh doanh', 'Phap luat', 'Suc khoe', 'The gioi', 'The thao', 'Van hoa', 'Vi tinh']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_all_categories(data_directory):\n",
    "    # List all directories (which represent categories) in the data folder\n",
    "    categories = [folder for folder in os.listdir(data_directory) if os.path.isdir(os.path.join(data_directory, folder))]\n",
    "    return categories\n",
    "\n",
    "# Path to the dataset\n",
    "data_directory = 'data/Train_Full'\n",
    "\n",
    "# Get the list of all categories (topics)\n",
    "categories = get_all_categories(data_directory)\n",
    "\n",
    "# Print the categories\n",
    "print(\"List of all categories (topics):\")\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1f72ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vntc_data_with_labels(data_directory):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    categories = get_all_categories(data_directory)  # Get the list of all categories\n",
    "\n",
    "    for category in categories:\n",
    "        category_path = os.path.join(data_directory, category)\n",
    "\n",
    "        for filename in os.listdir(category_path):\n",
    "            file_path = os.path.join(category_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-16') as file:\n",
    "                content = file.read().strip()\n",
    "                texts.append(content)\n",
    "                labels.append(category)  # Append category as label\n",
    "\n",
    "    return pd.DataFrame({'Labels': labels, 'Text': texts})\n",
    "\n",
    "# Load the data with labels\n",
    "data_train_directory = 'data/Train_Full'\n",
    "data_test_directory = 'data/Test_Full'\n",
    "data_train = load_vntc_data_with_labels(data_train_directory)\n",
    "data_test = load_vntc_data_with_labels(data_test_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77099401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Thành lập dự án POLICY phòng chống HIV/AIDS ở ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Hơn 16.000 khách đến vịnh Nha Trang Theo trực ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>TPHCM: Khai trương dịch vụ lặn biển săn cá mập...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Du lịch VN sẽ có tư vấn nước ngoài Ông Phạm Từ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Quy chế tuyển sinh 2006: Không làm tròn điểm t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33754</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Điện thoại di động tương lai trông như thế nào...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33755</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Internet sẽ tăng tốc 1.000 lần \\nTrong tương l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33756</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Phần lớn thế giới thứ 3 thất bại với chính phủ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33757</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>'Doom 3' giành chiến thắng kép \\nTrò chơi hành...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33758</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Lỗi về Window Media Player (1)\\nTôi đang dùng ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33759 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Labels                                               Text\n",
       "0      Chinh tri Xa hoi  Thành lập dự án POLICY phòng chống HIV/AIDS ở ...\n",
       "1      Chinh tri Xa hoi  Hơn 16.000 khách đến vịnh Nha Trang Theo trực ...\n",
       "2      Chinh tri Xa hoi  TPHCM: Khai trương dịch vụ lặn biển săn cá mập...\n",
       "3      Chinh tri Xa hoi  Du lịch VN sẽ có tư vấn nước ngoài Ông Phạm Từ...\n",
       "4      Chinh tri Xa hoi  Quy chế tuyển sinh 2006: Không làm tròn điểm t...\n",
       "...                 ...                                                ...\n",
       "33754           Vi tinh  Điện thoại di động tương lai trông như thế nào...\n",
       "33755           Vi tinh  Internet sẽ tăng tốc 1.000 lần \\nTrong tương l...\n",
       "33756           Vi tinh  Phần lớn thế giới thứ 3 thất bại với chính phủ...\n",
       "33757           Vi tinh  'Doom 3' giành chiến thắng kép \\nTrò chơi hành...\n",
       "33758           Vi tinh  Lỗi về Window Media Player (1)\\nTôi đang dùng ...\n",
       "\n",
       "[33759 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb01e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Mạo hiểm rừng Đa Mi Cuộc hành quân khám phá th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Tàu du lịch cao tốc Cần Thơ - Phnom Penh Công ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Miền Trung được mùa khách Thái Đoàn du khách T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>7 kỳ quan mới của thế giới Cầu Akashi - Kaikyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Khối A thi được mấy trường? Thi khối A vào ĐH ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50368</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Phần mềm chống tải nhạc bất hợp pháp\\nNhóm ngh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50369</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Linux tăng ảnh hưởng trên thị trường máy chủ\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50370</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Napster phát không máy nghe nhạc\\nCông ty phát...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50371</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Intel giới thiệu chip và chipset chuyên biệt h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50372</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>Yahoo lại bị trì trệ\\nLần thứ 2 trong chưa đầy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50373 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Labels                                               Text\n",
       "0      Chinh tri Xa hoi  Mạo hiểm rừng Đa Mi Cuộc hành quân khám phá th...\n",
       "1      Chinh tri Xa hoi  Tàu du lịch cao tốc Cần Thơ - Phnom Penh Công ...\n",
       "2      Chinh tri Xa hoi  Miền Trung được mùa khách Thái Đoàn du khách T...\n",
       "3      Chinh tri Xa hoi  7 kỳ quan mới của thế giới Cầu Akashi - Kaikyo...\n",
       "4      Chinh tri Xa hoi  Khối A thi được mấy trường? Thi khối A vào ĐH ...\n",
       "...                 ...                                                ...\n",
       "50368           Vi tinh  Phần mềm chống tải nhạc bất hợp pháp\\nNhóm ngh...\n",
       "50369           Vi tinh  Linux tăng ảnh hưởng trên thị trường máy chủ\\n...\n",
       "50370           Vi tinh  Napster phát không máy nghe nhạc\\nCông ty phát...\n",
       "50371           Vi tinh  Intel giới thiệu chip và chipset chuyên biệt h...\n",
       "50372           Vi tinh  Yahoo lại bị trì trệ\\nLần thứ 2 trong chưa đầy...\n",
       "\n",
       "[50373 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b659f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Labels\n",
       "Chinh tri Xa hoi    7567\n",
       "The gioi            6716\n",
       "The thao            6667\n",
       "Van hoa             6250\n",
       "Suc khoe            5417\n",
       "Kinh doanh          5276\n",
       "Vi tinh             4560\n",
       "Phap luat           3788\n",
       "Khoa hoc            2096\n",
       "Doi song            2036\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['Labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0673c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from underthesea import text_normalize, word_tokenize\n",
    "\n",
    "class Preprocessing:\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.stopwords = self.read_stopwords(file_path)\n",
    "\n",
    "    def read_stopwords(self, file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            stopwords = set(file.read().splitlines())\n",
    "        return stopwords\n",
    "    \n",
    "    def preprocessing_text(self, text):\n",
    "        text = text.lower().strip()\n",
    "\n",
    "        text = re.sub(r'\\b(\\d+)k\\b', r'\\1 ngàn', text)\n",
    "        text = re.sub(r'\\b(\\d+)%\\b', r'\\1 phần trăm', text)\n",
    "        text = re.sub(r'\\b(\\d+)m\\b', r'\\1 mét', text)\n",
    "        text = re.sub(r'\\b(\\d+)s\\b', r'\\1 giây', text)\n",
    "        text = re.sub(r\"\\b(\\d+)'\\b\", r'\\1 phút', text)\n",
    "        text = re.sub(r'\\b(\\d+)h\\b', r'\\1 giờ', text)\n",
    "\n",
    "        text = re.sub(r'[^0-9a-zA-ZÀ-ỹ\\s]', '', text)\n",
    "\n",
    "        text = text_normalize(text)\n",
    "        words = word_tokenize(text, format=\"text\").split()\n",
    "        \n",
    "        words = [w for w in words if w not in self.stopwords]\n",
    "\n",
    "        return \" \".join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1840a3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_train.sample(n=500).reset_index(drop=True)\n",
    "data_test = data_test.sample(n=300).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2022d0aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suc khoe</td>\n",
       "      <td>Mùa hè ăn gì để đẹp da?\\nTheo Đông y, về mùa h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phap luat</td>\n",
       "      <td>Án mạng nghiêm trọng từ chuyện nợ nần\\nTối mùn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>ADSL chinh phục người dùng bằng tốc độ\\nVới tố...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phap luat</td>\n",
       "      <td>Hà Kiều Anh vẫn yêu cầu đòi bồi thường 20.000 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The gioi</td>\n",
       "      <td>Hôm 9/3, Trung Quốc đã trả đũa các chỉ trích n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Ronaldo tiếp tục lập công để dập tắt mọi mối n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Lehmann đưa Arsenal vào chung kết Thủ thành Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Tuyển đại lý cầu thủ của FIFA: hai thí sinh đề...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Như chỗ không người “Tè đường” trở thành căn b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Kinh doanh</td>\n",
       "      <td>Vay tiêu dùng tăng mạnh\\nTheo các ngân hàng tr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Labels                                               Text\n",
       "0            Suc khoe  Mùa hè ăn gì để đẹp da?\\nTheo Đông y, về mùa h...\n",
       "1           Phap luat  Án mạng nghiêm trọng từ chuyện nợ nần\\nTối mùn...\n",
       "2             Vi tinh  ADSL chinh phục người dùng bằng tốc độ\\nVới tố...\n",
       "3           Phap luat  Hà Kiều Anh vẫn yêu cầu đòi bồi thường 20.000 ...\n",
       "4            The gioi  Hôm 9/3, Trung Quốc đã trả đũa các chỉ trích n...\n",
       "..                ...                                                ...\n",
       "495          The thao  Ronaldo tiếp tục lập công để dập tắt mọi mối n...\n",
       "496          The thao  Lehmann đưa Arsenal vào chung kết Thủ thành Le...\n",
       "497          The thao  Tuyển đại lý cầu thủ của FIFA: hai thí sinh đề...\n",
       "498  Chinh tri Xa hoi  Như chỗ không người “Tè đường” trở thành căn b...\n",
       "499        Kinh doanh  Vay tiêu dùng tăng mạnh\\nTheo các ngân hàng tr...\n",
       "\n",
       "[500 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d67d3294",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='vietnamese-stopwords-dash.txt'\n",
    "preprocessor = Preprocessing(file_path)\n",
    "data_train['Text_Processing'] = data_train['Text'].apply(preprocessor.preprocessing_text)\n",
    "data_test['Text_Processing'] = data_test['Text'].apply(preprocessor.preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "465d2b14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Labels</th>\n",
       "      <th>Text</th>\n",
       "      <th>Text_Processing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suc khoe</td>\n",
       "      <td>Mùa hè ăn gì để đẹp da?\\nTheo Đông y, về mùa h...</td>\n",
       "      <td>mùa hè đẹp da đông_y mùa hè thức_ăn_nhiệt rôm ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phap luat</td>\n",
       "      <td>Án mạng nghiêm trọng từ chuyện nợ nần\\nTối mùn...</td>\n",
       "      <td>án_mạng nghiêm_trọng nợ_nần tối mùng 5 tết 261...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vi tinh</td>\n",
       "      <td>ADSL chinh phục người dùng bằng tốc độ\\nVới tố...</td>\n",
       "      <td>adsl chinh_phục tốc_độ tốc_độ đường truyền gấp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phap luat</td>\n",
       "      <td>Hà Kiều Anh vẫn yêu cầu đòi bồi thường 20.000 ...</td>\n",
       "      <td>hà_kiều đòi bồi_thường 20000 usd trưa tand tỉn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The gioi</td>\n",
       "      <td>Hôm 9/3, Trung Quốc đã trả đũa các chỉ trích n...</td>\n",
       "      <td>hôm 93 trung_quốc trả_đũa chỉ_trích nhân_quyền...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Ronaldo tiếp tục lập công để dập tắt mọi mối n...</td>\n",
       "      <td>ronaldo lập_công dập tắt nghi_ngờ phong_độ đội...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Lehmann đưa Arsenal vào chung kết Thủ thành Le...</td>\n",
       "      <td>lehmann arsenal chung_kết_thủ thành lehmann đồ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>The thao</td>\n",
       "      <td>Tuyển đại lý cầu thủ của FIFA: hai thí sinh đề...</td>\n",
       "      <td>tuyển đại_lý cầu_thủ fifa hai thí_sinh thất_bạ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Chinh tri Xa hoi</td>\n",
       "      <td>Như chỗ không người “Tè đường” trở thành căn b...</td>\n",
       "      <td>chỗ tè đường căn_bệnh truyền_nhiễm văn_minh ph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Kinh doanh</td>\n",
       "      <td>Vay tiêu dùng tăng mạnh\\nTheo các ngân hàng tr...</td>\n",
       "      <td>vay tiêu_dùng ngân_hàng địa_bàn tp hcm khách_h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Labels                                               Text  \\\n",
       "0            Suc khoe  Mùa hè ăn gì để đẹp da?\\nTheo Đông y, về mùa h...   \n",
       "1           Phap luat  Án mạng nghiêm trọng từ chuyện nợ nần\\nTối mùn...   \n",
       "2             Vi tinh  ADSL chinh phục người dùng bằng tốc độ\\nVới tố...   \n",
       "3           Phap luat  Hà Kiều Anh vẫn yêu cầu đòi bồi thường 20.000 ...   \n",
       "4            The gioi  Hôm 9/3, Trung Quốc đã trả đũa các chỉ trích n...   \n",
       "..                ...                                                ...   \n",
       "495          The thao  Ronaldo tiếp tục lập công để dập tắt mọi mối n...   \n",
       "496          The thao  Lehmann đưa Arsenal vào chung kết Thủ thành Le...   \n",
       "497          The thao  Tuyển đại lý cầu thủ của FIFA: hai thí sinh đề...   \n",
       "498  Chinh tri Xa hoi  Như chỗ không người “Tè đường” trở thành căn b...   \n",
       "499        Kinh doanh  Vay tiêu dùng tăng mạnh\\nTheo các ngân hàng tr...   \n",
       "\n",
       "                                       Text_Processing  \n",
       "0    mùa hè đẹp da đông_y mùa hè thức_ăn_nhiệt rôm ...  \n",
       "1    án_mạng nghiêm_trọng nợ_nần tối mùng 5 tết 261...  \n",
       "2    adsl chinh_phục tốc_độ tốc_độ đường truyền gấp...  \n",
       "3    hà_kiều đòi bồi_thường 20000 usd trưa tand tỉn...  \n",
       "4    hôm 93 trung_quốc trả_đũa chỉ_trích nhân_quyền...  \n",
       "..                                                 ...  \n",
       "495  ronaldo lập_công dập tắt nghi_ngờ phong_độ đội...  \n",
       "496  lehmann arsenal chung_kết_thủ thành lehmann đồ...  \n",
       "497  tuyển đại_lý cầu_thủ fifa hai thí_sinh thất_bạ...  \n",
       "498  chỗ tè đường căn_bệnh truyền_nhiễm văn_minh ph...  \n",
       "499  vay tiêu_dùng ngân_hàng địa_bàn tp hcm khách_h...  \n",
       "\n",
       "[500 rows x 3 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de536c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from underthesea import text_normalize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3abfb8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "data_train['Labels'] = label_encoder.fit_transform(data_train['Labels'])\n",
    "data_test['Labels'] = label_encoder.transform(data_test['Labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5004ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def tokenizer_fn(s: str):\n",
    "    return word_tokenize(s, format=\"text\").split()\n",
    "\n",
    "def build_vocab_from_texts(text_series, min_freq=1, specials=(\"<unk>\", \"<pad>\")):\n",
    "    counter = Counter()\n",
    "    for s in text_series:\n",
    "        counter.update(tokenizer_fn(s))\n",
    "    itos = list(specials)  # index -> token\n",
    "    for tok, freq in counter.most_common():\n",
    "        if freq >= min_freq and tok not in specials:\n",
    "            itos.append(tok)\n",
    "    stoi = {tok: i for i, tok in enumerate(itos)}  # token -> index\n",
    "    return stoi, itos\n",
    "\n",
    "stoi, itos = build_vocab_from_texts(data_train['Text_Processing'], min_freq=1)\n",
    "UNK_IDX = stoi[\"<unk>\"]\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "\n",
    "def text_pipeline(x: str):\n",
    "    return [stoi.get(tok, UNK_IDX) for tok in tokenizer_fn(x)]\n",
    "\n",
    "def label_pipeline(y):\n",
    "    return int(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac07f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17819, 86835)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from itertools import chain\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def texts_to_ids(text_series):\n",
    "    seqs = [text_pipeline(s) for s in text_series]\n",
    "    flat = list(chain.from_iterable(seqs))\n",
    "    return flat\n",
    "\n",
    "corpus_ids = texts_to_ids(data_train['Text_Processing'])\n",
    "\n",
    "class NextTokenDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=50, step=1):\n",
    "        self.token_ids = token_ids\n",
    "        self.seq_len = seq_len\n",
    "        self.step = step\n",
    "        self.n_samples = max(0, (len(token_ids) - 1 - seq_len) // step + 1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.step\n",
    "        x = torch.tensor(self.token_ids[i:i+self.seq_len], dtype=torch.long)\n",
    "        y = torch.tensor(self.token_ids[i+1:i+self.seq_len+1], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "SEQ_LEN = 50\n",
    "BATCH_SIZE = 64\n",
    "dataset = NextTokenDataset(corpus_ids, seq_len=SEQ_LEN, step=1)\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "len_vocab = len(itos)\n",
    "len_vocab, len(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=256, hidden_dim=512, num_layers=2, dropout=0.2, pad_idx=PAD_IDX):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=emb_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        # x: [B, T]\n",
    "        emb = self.emb(x)           # [B, T, E]\n",
    "        out, hidden = self.lstm(emb, hidden)  # out: [B, T, H]\n",
    "        out = self.drop(out)\n",
    "        logits = self.fc(out)       # [B, T, V]\n",
    "        return logits, hidden\n",
    "\n",
    "model = LSTMLanguageModel(vocab_size=len_vocab, emb_dim=256, hidden_dim=512, num_layers=2, dropout=0.3).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0062a863",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m1\u001b[39m, total_tokens), ppl\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 33\u001b[0m     avg_loss, ppl \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | loss/token = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | ppl = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mppl\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[19], line 19\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, loader, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     17\u001b[0m logits, _ \u001b[38;5;241m=\u001b[39m model(x)  \u001b[38;5;66;03m# [B, T, V]\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), y\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m clip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ThienLaptop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    646\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ThienLaptop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ThienLaptop\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    830\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    831\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 2e-3\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss, total_tokens = 0.0, 0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)  # [B, T]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, _ = model(x)  # [B, T, V]\n",
    "        loss = criterion(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tokens = (y != PAD_IDX).sum().item()\n",
    "            total_loss += loss.item() * tokens\n",
    "            total_tokens += tokens\n",
    "\n",
    "    ppl = math.exp(total_loss / max(1, total_tokens))\n",
    "    return total_loss / max(1, total_tokens), ppl\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    avg_loss, ppl = train_one_epoch(model, loader, optimizer, criterion, clip=1.0)\n",
    "    print(f\"Epoch {epoch:02d} | loss/token = {avg_loss:.4f} | ppl = {ppl:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def top_k_filtering(logits, top_k=0):\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(1)\n",
    "        logits = torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "    return logits\n",
    "\n",
    "def generate_text(\n",
    "    prompt: str,\n",
    "    max_new_tokens: int = 80,\n",
    "    temperature: float = 1.0,\n",
    "    top_k: int = 0\n",
    "):\n",
    "    model.eval()\n",
    "    ids = [stoi.get(tok, UNK_IDX) for tok in tokenizer_fn(prompt)]\n",
    "    x = torch.tensor(ids, dtype=torch.long, device=device).unsqueeze(0)  # [1, T]\n",
    "    hidden = None\n",
    "    generated = ids[:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, hidden = model(x[:, -SEQ_LEN:], hidden)  \n",
    "            next_logits = logits[:, -1, :]  # [1, V]\n",
    "            next_logits = next_logits / max(1e-6, temperature)\n",
    "            next_logits = top_k_filtering(next_logits, top_k=top_k)\n",
    "            probs = F.softmax(next_logits, dim=-1)\n",
    "            next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "            generated.append(next_id)\n",
    "            x = torch.tensor([generated], dtype=torch.long, device=device)\n",
    "\n",
    "    return \" \".join(itos[idx] for idx in generated)\n",
    "\n",
    "seed = \"sữa chua uống men sống nuvi là sản phẩm\"\n",
    "print(generate_text(seed, max_new_tokens=40, temperature=0.9, top_k=50))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
